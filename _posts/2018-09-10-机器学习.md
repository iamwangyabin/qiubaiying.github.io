---
layout:     post
title:      机器学习
subtitle:   读书笔记
date:       2018-06-05
author:     WYB
header-img: img/post-bg-cook.jpg
catalog: true
tags:
    - 机器学习
---

# ![image](https://note.youdao.com/yws/public/resource/69b6221d64d88f5861bbae991b46b2a6/xmlnote/WEBRESOURCEd6e75b41ca5770417617f1e5b2dffea1/1774)

简单model的Variance比较小，复杂的model散布比较开。

如果model无法fit training data 应该是bias大了。

如果是在训练集好而测试集不好，应该是large variance。

for bias redesign your model:
1. Add more features as input
2. A more complex model

for large variance:
1. More data
2. Regularization

# 入门
机器学习其实就是找一个方程，以前都是人类自己找方程去描述解释自然，但是当问题非常复杂时候靠人类去找几乎不可能的。像是深度学习过程就是从无穷的方程中不断衡量最后确定一个最好的方程。

# 2、模型评估与选择
### 2.1 经验误差与过拟合
经验误差是对训练集做的误差，泛化误差是新样本的误差。
##### 2.2.1 留出法
就是对训练集划分出一些来测试，一般要多次留出才能保真。
##### 2.2.2 交叉验证法
##### 2.2.3 自助法
### 2.3 性能度量
衡量模型泛化能力的标准



# 3、线性模型
### 3.1 基本形式
线性模型有很好的可解释性
```math
f(x)=w_1x_1+w_2x_2+...+w_dx_d+b

f(x)=w^Tx+b
```
### 3.2 线性回归
线性回归试图学得一种拟合样本的函数


对于离散属性，如属性间存在order，通过连续化将其转化为连续值；不存在order的，通常转化为k维向量。 

关键在于衡量预测值同真实值的差别，我们试图让均方误差最小化

```math
(w^*,b^*)=argmin
```
### 3.3 对数几率回归
通过回归来进行分类

# 4、朴素贝叶斯

P(W|C)就是C事件发生的前提下W事件发生的概率,P(C)是事件C发生的概率。

条件概率的定义：
        P(W|C)=P(CW)/P(C)
    
贝叶斯公式：
    P(A∩B)=P(A)*P(B|A)=P(B)*P(A|B)
    P(A|B)=P(B|A)*P(A)/P(B) 

set()可以把列表转换成集合{}，集合特点可以消除重复元素，再用list()转换成列表

字符串 index()方法

str.index(str, beg=0, end=len(string))

如果包含子字符串返回开始的索引值，否则抛出异常。
    

# 6、支持向量机
### 6.1 间隔与支持向量
#### 线性二分类模型：
就是找到一组参数（w，b）使得：∀i. yi(w⊤xi + b) > 0
#### 划分超平面可用如下线性方程描述：

```math
w^Tx+b=0
```
w是法向量，b是超平面距离原点距离。
#### 样本空间任一点x距离超平面距离为：

```math
r=\frac{|w^T+b|}{||w||}
```
#### 间隔：

```math
\gamma=\frac{2}{||w||}
```
支持向量机就是为了找到最大间隔，即：

```math
\mathrm{max}_{w,b}\frac{2}{||w||}

s.t\;y_i(w^Tx_i+b)\ge1

```
等效最小化间隔：
```math
\mathrm{min}_{w,b}\frac{1}{2}{||w||}^2

s.t\;y_i(w^Tx_i+b)\ge1

```
### 6.2 对偶问题
就是用拉格朗日乘子法让最小化问题编程多参数无限制条件问题。  
等效对偶问题为：
```math
\mathrm{max}_{\alpha}\sum_{i=1}^m {\alpha}_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m {\alpha}_i{\alpha}_j y_iy_jx_i^Tx_j

s.t.\;\sum_{i=1}^m{\alpha}_iy_i=0
```

### 6.3 核函数
如果样本空间有限维，则一定存在一个高维特征空间使样本可分。

```math
E = mc^2
```

# 8、集成学习
### 8.1、个体与集成
1. 同质集成（homogeneous）的个体学习器是‘基学习器’（based learner）相应学习算法是‘基学习算法’
2. 异质集成（heterogeneous）的个体学习器是‘组件学习器’（component learner）
3. 集成可以获得比单一学习器更好的==泛化能力==
4. 好的集成需要个体学习器“好而不同”，既要有一定准确性还要有“多样性”
5. “好而不同”这是个矛盾，正是集成学习的研究核心
6. 分为两类：个体学习器之间存在强依赖关系（Boosting）和不存在强依赖关系（Random Forest）

### 8.2、Boosting
1. 先从初始训练集训练出一个基学习器
2. 根据基学习器对训练样本进行调整，使得先前做错的样本在后续收到更多关注
3. 基于新样本再训练一个基学习器
4. 重复，直到基学习器数量达到指定值
5. 把所有基学习器进行加权结合

####   AdaBoost算法
输入：训练集D={(),(),...,()};