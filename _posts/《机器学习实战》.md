# 4、朴素贝叶斯
    P(W|C)就是C事件发生的前提下W事件发生的概率,P(C)是事件C发生的概率。
    条件概率的定义：
        P(W|C)=P(CW)/P(C)
    贝叶斯公式：
        P(A∩B)=P(A)*P(B|A)=P(B)*P(A|B)
        P(A|B)=P(B|A)*P(A)/P(B)    
--------------------------------------------------------------------------------

        set()可以把列表转换成集合{}，集合特点可以消除重复元素，再用list()转换成列表。
        字符串 index()方法
        str.index(str, beg=0, end=len(string))
        如果包含子字符串返回开始的索引值，否则抛出异常。
    

# 6、支持向量机
#### 线性二分类模型：
就是找到一组参数（w，b）使得：∀i. yi(w⊤xi + b) > 0
##### 间隔：
引理 2.
 R d 空间中某点 p ∈ R d 到超平面 w⊤x + b = 0 的距离为 1 /||w||×|w⊤p + b| .
||
# 8、集成学习
### 8.1、个体与集成
1. 同质集成（homogeneous）的个体学习器是‘基学习器’（based learner）相应学习算法是‘基学习算法’
2. 异质集成（heterogeneous）的个体学习器是‘组件学习器’（component learner）
3. 集成可以获得比单一学习器更好的==泛化能力==
4. 好的集成需要个体学习器“好而不同”，既要有一定准确性还要有“多样性”
5. “好而不同”这是个矛盾，正是集成学习的研究核心
6. 分为两类：个体学习器之间存在强依赖关系（Boosting）和不存在强依赖关系（Random Forest）

### 8.2、Boosting
1. 先从初始训练集训练出一个基学习器
2. 根据基学习器对训练样本进行调整，使得先前做错的样本在后续收到更多关注
3. 基于新样本再训练一个基学习器
4. 重复，直到基学习器数量达到指定值
5. 把所有基学习器进行加权结合

####   AdaBoost算法
输入：训练集D={(),(),...,()};


# 回归
### 用线性回归找到最佳拟合曲线

求回归系数的过程就是回归

对于给定数据X，回归系数W，结果Y，求回归就是求平方误差最小的W

平方误差可以写作：

```math
\sum_{i=1}^m (y_i-x_i^Tw)^2
```
对w求导，令为零，可以解得：

```math
\hat{w}=(X^TX)^{-1}X^Ty
```
这个也称为普通最小二乘法

求的是具有最小均方误差的无偏估计，有可能出现欠拟合。

### 局部加权线性回归
在估计中加入一些偏差，从而降低预测的均方误差。

LWLR给待预测点附近每个点赋予一定的权重


```math
\hat w=(X^TWX)^{-1}X^TWy
```
W是一个矩阵，用来给每个数据点赋予权重

LWLR用核函数对附近点赋予更高的权重，最常见的是高斯核：


```math
w(i,i)=\mathrm{exp} (\dfrac{|x^{(i)}-x|}{-2k^2})
```
k是用户定义参数，决定给附近的点赋予多大的权重
